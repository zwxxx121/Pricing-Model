{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9ec5293-8021-483b-8da5-84bddd64c530",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import python & PySpark Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af0a4910-d453-4fbb-ab99-6dfab26b92a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, to_date, year, sum, avg, max, desc, row_number, when, weekofyear, explode, format_string, round\n",
    "from pyspark.sql.window import Window\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import catboost as cbt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a232ffd-6f79-49f0-a414-93f9595a5dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9d60228-1d7e-4849-a452-8bb5107f14ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import product and transaction data\n",
    "prod = spark.table(\"pro_mrg\")\n",
    "trxn = spark.table(\"transaction\")\n",
    "# convert trans_dt to date format, convert other numerical variables to double format\n",
    "trxn = trxn.withColumn(\"trans_dt\", to_date(col('trans_dt'), \"M/d/yyyy\"))\n",
    "trxn = trxn.withColumn(\"sales_amt\", col('sales_amt').cast('double'))\n",
    "trxn = trxn.withColumn(\"sales_qty\", col('sales_qty').cast('double'))\n",
    "trxn = trxn.withColumn(\"sales_wgt\", col('sales_wgt').cast('double'))\n",
    "# extract week number from trans_dt \n",
    "trxn = trxn.withColumn('week', weekofyear('trans_dt'))\n",
    "# drop abnormal data\n",
    "trxn = trxn.filter((trxn.sales_amt>=0)&(trxn.sales_qty>=0)&(trxn.sales_wgt>=0))\n",
    "# add a column of unit price (determined by qty or wgt)\n",
    "trxn = trxn.withColumn(\"price\", when(trxn[\"sales_wgt\"] == 0, trxn[\"sales_amt\"] / trxn[\"sales_qty\"]).otherwise(trxn[\"sales_amt\"] / trxn[\"sales_wgt\"]))\n",
    "# filter transaction in 2020 to reduce uncontrollable factors across years, meanwhile enhance computing efficiency; join two tables.\n",
    "trxn = trxn.filter(year(trxn.trans_dt)==2020)\n",
    "trxn = trxn.join(prod, on='prod_id', how='inner')\n",
    "# calculate sales of each product and category\n",
    "prod_sales = trxn.select(['prod_id','sales_qty']).groupBy('prod_id').agg(sum('sales_qty').alias('total_sales'))\n",
    "cat_sales = trxn.select(['sales_qty','prod_category']).groupBy('prod_category').agg(sum('sales_qty').alias('total_sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d38f5b7e-cc39-4d95-a3e6-bc2934cbb1e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sort product by descending sales; pick the top500 products of highest demand\n",
    "total_prod_sales = prod_sales.select(sum('total_sales')).rdd.flatMap(lambda x: x).collect()[0]\n",
    "prod_sales = prod_sales.withColumn(\"sales_percent\", sum(prod_sales.total_sales).over(Window.orderBy(desc('total_sales')))/total_prod_sales)\n",
    "# prod_sales.filter(prod_sales.sales_percent>0.8).count()/prod_sales.count() \n",
    "# 20% demands consist of 90% products, so we can choose main products to improve efficiency\n",
    "prod_500 = prod_sales.limit(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa41abed-c641-4862-a101-11ffaba7033a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weekly avg_price and demand of each product\n",
    "prod_500 = prod_500.select('prod_id').join(trxn, on='prod_id', how='inner')\n",
    "weekly_prod = prod_500.groupBy(['prod_id','week']).agg(avg('price').alias('weekly_price'),\n",
    "                                                      sum('sales_qty').alias('weekly_demand'))\n",
    "weekly_prod = weekly_prod.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297c6f7f-4a34-4884-8d94-77aa5939ae7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Constant Elasticity Model - Price Response Function\n",
    "\n",
    "Formula: log(demand/week)~log(price/week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4c2141d-c8f2-4316-9724-f815012d9a19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# weekly demand\n",
    "prod_sales_df = weekly_prod.groupby('prod_id',as_index=False).weekly_demand.sum()\n",
    "# log the both sides of price response function\n",
    "weekly_prod['logP'] = np.log(weekly_prod[\"weekly_price\"])\n",
    "weekly_prod['logD'] = np.log(weekly_prod[\"weekly_demand\"])\n",
    "# use smf.ols regression to compute p-value and coefficient\n",
    "# Analyze elasticity based on p-value and coefficient of each product.\n",
    "prod = weekly_prod.prod_id.unique()\n",
    "prod_elastisity = pd.DataFrame(columns = ['prod_id','elasticity','p_value'])\n",
    "for i in prod:\n",
    "    sample = weekly_prod[weekly_prod.prod_id==i]\n",
    "    result = smf.ols('logD ~ logP', data=sample).fit()\n",
    "    intercept, slope = result.params\n",
    "    slope = float('{0:.2f}'.format(slope))\n",
    "    p = float('{0:.2f}'.format(result.f_pvalue))\n",
    "    new_row = pd.DataFrame(np.array([[i,slope,p]]),columns=['prod_id','elasticity','p_value'])\n",
    "    prod_elastisity = pd.concat([prod_elastisity,new_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feaa5bb6-887b-4f93-bb11-8a7c293d42c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# choose 15 Elastic products for EDLP strategy \n",
    "# p<0.05 shows demand is significantly affected by price; elasticity<-1; choose products with high demands\n",
    "prod_sales_df = weekly_prod.groupby('prod_id',as_index=False).weekly_demand.sum()\n",
    "elastic = prod_elastisity[(prod_elastisity.elasticity<-1)&(prod_elastisity.p_value<0.05)].merge(prod_sales_df,on='prod_id').sort_values(['weekly_demand'])[0:15]\n",
    "# choose 80 inElastic products for Hi-Lo strategy \n",
    "# -1<elasticity<0; choose products with high demands \n",
    "# small increase in price won't cause too much decrease in quantity demand\n",
    "inelastic = prod_elastisity[(prod_elastisity.elasticity>-1)&(prod_elastisity.elasticity<0)].merge(prod_sales_df,on='prod_id').sort_values(['weekly_demand'])[0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff181ace-7903-48e2-98ad-8e3b329b06ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot demand-price of EDLP\n",
    "k = elastic.prod_id.unique()[0]\n",
    "plt.plot(weekly_prod[weekly_prod.prod_id==k].sort_values('weekly_price').weekly_price, \n",
    "         weekly_prod[weekly_prod.prod_id==k].sort_values('weekly_price').weekly_demand)\n",
    "plt.title('product %i price - demand'%k)\n",
    "plt.xlabel('weekly price')\n",
    "plt.ylabel('weekly demand')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c237d7a-50d0-41a5-8e86-9e04e36568a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Find complementary item for each product using basket analysis and calculate the weekly price of complementary item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a296360-618a-447e-bda3-42efc27a8d19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "basketdata = trxn.select('prod_id','trans_id').dropDuplicates(['prod_id','trans_id']).sort('trans_id')\n",
    "basketdata = basketdata.groupBy('trans_id').agg(F.collect_list('prod_id')).sort('trans_id')\n",
    "# Frequent Pattern Growth â€“ FP Growth is a method of mining frequent itemsets using support, lift, and confidence.\n",
    "fpg = FPGrowth(itemsCol=\"collect_list(prod_id)\", minSupport=0.005, minConfidence=0.005)\n",
    "model = fpg.fit(basketdata)\n",
    "# sort by descending suppport/confidentce/lift, and filter antecedents that are 95 products we chose\n",
    "rules = model.associationRules.sort(desc(\"support\"), desc(\"confidence\"), desc(\"lift\"))\n",
    "prod_60 = spark.createDataFrame(pd.DataFrame(pd.concat([elastic, inelastic], ignore_index=True).prod_id))\n",
    "prod_60 = prod_60.withColumn(\"prod_id\", col(\"prod_id\").cast(\"long\"))\n",
    "rules = rules.select(explode('antecedent').alias('antecedent'), 'consequent', 'support','confidence','lift')\n",
    "rules = rules.withColumnRenamed('antecedent', 'prod_id')\n",
    "rules = rules.join(prod_60, 'prod_id', \"inner\")\n",
    "rules.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cee63611-7fd4-4c2a-81fc-8d0de8ddc1a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# choose 1 complementary for each product\n",
    "w3 = Window.partitionBy('prod_id').orderBy(desc(\"support\"), desc(\"confidence\"), desc(\"lift\"))\n",
    "rules = rules.withColumn('row_num', row_number().over(w3))\n",
    "rules = rules.filter(rules.row_num <= 1).drop('row_num')\n",
    "rules = rules.withColumnRenamed('prod_id', 'prod_60')\n",
    "rules = rules.select('prod_60', explode('consequent').alias('prod_id'))\n",
    "rules = rules.withColumn('prod_id', col('prod_id').cast('long'))\n",
    "rules = rules.join(trxn.select(['prod_id','week','price']),on='prod_id', how='inner')\n",
    "complementary = rules.groupBy(['prod_id','prod_60','week']).agg(avg('price').alias('avg_comp_price'))\n",
    "complementary = complementary.select('prod_60','prod_id','avg_comp_price','week')\n",
    "complementary = complementary.toPandas()\n",
    "# pivot the table into [week, prod_id, comp_weekly_price]\n",
    "complementary_pivot = pd.DataFrame(columns = ['week','comp','prod_60'])\n",
    "for i in complementary.prod_60.unique():\n",
    "    #print(i)\n",
    "    pivot = complementary[complementary.prod_60==i].pivot(index='week', columns='prod_id', values='avg_comp_price').reset_index().iloc[:,:2] # prod_id>prod_60[i]\n",
    "    pivot.columns = ['week','comp']\n",
    "    pivot['prod_60']=i\n",
    "    complementary_pivot = pd.concat([complementary_pivot, pivot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ee76b21-f582-4c29-9bcc-a54c1b7d25a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Find substitute item for each product in the same subcategory with same type/prod_unit_qty_count/prod_count_uom and calculate the weekly price of substitute item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d712b62-689f-4990-a839-e061bc203f4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prod = prod.withColumn('prod_id', col('prod_id').cast('long'))\n",
    "prod_60 = spark.createDataFrame(pd.DataFrame(pd.concat([elastic, inelastic], ignore_index=True).prod_id))\n",
    "prod_60 = prod_60.withColumn(\"prod_id\", col(\"prod_id\").cast(\"long\"))\n",
    "prod_60 = prod_60.join(prod,on='prod_id')\n",
    "prod_60 = prod_60.withColumnRenamed('prod_id','prod_60')\n",
    "prod_60 = prod_60.withColumnRenamed('prod_desc','desc_60')\n",
    "prod_60 = prod_60.withColumnRenamed('prod_mfc_brand_cd','brand_60')\n",
    "substitute = prod_60.join(trxn.select('prod_id','trans_id','prod_desc','prod_type','prod_subcategory','prod_mfc_brand_cd','prod_unit_qty_count','prod_count_uom','price','week','sales_qty'), on=['prod_type','prod_subcategory','prod_unit_qty_count','prod_count_uom'])\n",
    "substitute = substitute.filter(substitute.prod_60!=substitute.prod_id)\n",
    "subs_total_sale = substitute.groupBy(['prod_60','prod_id']).agg(sum('sales_qty').alias('total_sales'))\n",
    "w4 = Window.partitionBy('prod_60').orderBy(desc('total_sales'))\n",
    "subs_total_sale = subs_total_sale.withColumn('row_num', row_number().over(w4))\n",
    "# only keep 1 substitutes with the largest sales\n",
    "subs_total_sale = subs_total_sale.filter(subs_total_sale.row_num <= 1).drop('row_num')\n",
    "substitute = substitute.join(subs_total_sale, on=['prod_60','prod_id'])\n",
    "substitute = substitute.select(['prod_60','prod_id','week','price']).groupBy(['prod_60','prod_id','week']).agg(avg('price').alias('avg_subs_price'))\n",
    "substitute = substitute.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e89ea6d2-2812-47b2-b85e-2c0c8b39600e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pivot the table into [week, prod_id, sub_weekly_price]\n",
    "substitute_pivot = pd.DataFrame(columns=['week','sub','prod_60'])\n",
    "for i in substitute.prod_60.unique():\n",
    "    pivot = substitute[substitute.prod_60==i].pivot(index='week', columns='prod_id', values='avg_subs_price').reset_index() # prod_id>prod_60[i]\n",
    "    pivot.columns = ['week','sub']\n",
    "    pivot['prod_60']=i\n",
    "    substitute_pivot = substitute_pivot.append(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f7aafe8-59d5-4594-903b-e20e80915255",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Consider seasonality as weekly revenue of the same category for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3fe735f-17d0-49cc-8440-e81c7b9a5619",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prod_60 = spark.createDataFrame(pd.DataFrame(pd.concat([elastic, inelastic], ignore_index=True).prod_id))\n",
    "prod_60 = prod_60.withColumn(\"prod_id\", col(\"prod_id\").cast(\"long\"))\n",
    "prod_60 = prod_60.join(prod.select('prod_id','prod_category'),on='prod_id')\n",
    "prod_60 = prod_60.withColumnRenamed('prod_id','prod_60')\n",
    "seasonality = prod_60.join(trxn.select('prod_id','trans_id','prod_category','sales_amt','week'), on=['prod_category'])\n",
    "seasonality = seasonality.groupBy(['prod_category','week']).agg(sum('sales_amt').alias('category_revenue'))\n",
    "seasonality = seasonality.select('prod_category','week','category_revenue').join(prod_60, on='prod_category')\n",
    "seasonality = seasonality.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3893f832-b914-4496-b29e-7a87c5801b94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Find competitors of each product(same subcategory/type but different brand) and calculate competitors' weekly average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ea9c25c-1e06-4466-8581-115073d8a9ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prod_60 = spark.createDataFrame(pd.DataFrame(pd.concat([elastic, inelastic], ignore_index=True).prod_id))\n",
    "prod_60 = prod_60.withColumn(\"prod_id\", col(\"prod_id\").cast(\"long\"))\n",
    "prod_60 = prod_60.join(prod,on='prod_id')\n",
    "prod_60 = prod_60.withColumnRenamed('prod_id','prod_60')\n",
    "prod_60 = prod_60.withColumnRenamed('prod_mfc_brand_cd','brand_60')\n",
    "competitor = prod_60.join(trxn.select('prod_id','trans_id','prod_type','prod_subcategory','prod_mfc_brand_cd','price','week'), on=['prod_type','prod_subcategory'])\n",
    "competitor = competitor.filter(competitor.brand_60!=competitor.prod_mfc_brand_cd)\n",
    "competitor = competitor.groupBy(['prod_60','prod_subcategory','week']).agg(avg('price').alias('competitor_weekly_price'))\n",
    "competitor = competitor.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29683859-0dc1-4bde-8f97-84c7a151da38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Join product weekly price, complementary weekly price, substitute weekly price, competitor weekly price, and seanality into one table for regression model of demand prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39b0df6e-7f34-477f-8ee4-a165296a6984",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "complementary_pivot = complementary_pivot.rename(columns={'prod_60': 'prod_id'})\n",
    "substitute_pivot = substitute_pivot.rename(columns={'prod_60': 'prod_id'})\n",
    "seasonality = seasonality.rename(columns={'prod_60': 'prod_id'})\n",
    "competitor = competitor.rename(columns={'prod_60': 'prod_id'})\n",
    "# join features into regression table\n",
    "# EDLP needs to consider competitor price on the product\n",
    "EDLP_df = elastic.merge(weekly_prod, on=['prod_id'], how='left').merge(substitute_pivot, on = ['prod_id','week'], how='left').merge(seasonality, on = ['prod_id','week'], how='left').merge(competitor, on = ['prod_id','week'], how='left').merge(complementary_pivot, on = ['prod_id','week'], how='left').drop(['elasticity','p_value','logP','logD', 'prod_category','prod_subcategory'],axis=1)\n",
    "# Hi-Lo\n",
    "HiLo_df = inelastic.merge(weekly_prod, on=['prod_id'], how='left').merge(substitute_pivot, on = ['prod_id','week'], how='left').merge(seasonality, on = ['prod_id','week'], how='left').merge(complementary_pivot, on = ['prod_id','week'], how='left').merge(competitor, on = ['prod_id','week'], how='left').drop(['elasticity','p_value','logP','logD', 'prod_category'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95333c27-09fc-4318-8230-59b0caa8d37d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# theoretical max demand: elasticity=-1   \n",
    "md_edlp = EDLP_df[['prod_id','weekly_demand']].groupby('prod_id',as_index=False).weekly_demand.max()\n",
    "md_edlp = md_edlp.rename(columns={'weekly_demand': 'max_demand'})\n",
    "EDLP_df = EDLP_df.merge(md_edlp, on='prod_id', how='left')\n",
    "\n",
    "md_hilo = HiLo_df[['prod_id','weekly_demand']].groupby('prod_id',as_index=False).weekly_demand.max()\n",
    "md_hilo = md_hilo.rename(columns={'weekly_demand': 'max_demand'})\n",
    "HiLo_df = HiLo_df.merge(md_hilo, on='prod_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2679a42d-0154-45cb-9270-7de797bb06dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Demand Prediction - Logit Price Response Function (EDLP & Hi-Lo)\n",
    "\n",
    "Target = log(dit/(ui-dit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89053103-3e62-425f-bdd8-d8ebd081d7e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill in missing value \n",
    "EDLP_df['sub'] = EDLP_df['sub'].fillna(EDLP_df['sub'].mean())\n",
    "EDLP_df['competitor_weekly_price'] = EDLP_df['competitor_weekly_price'].fillna(EDLP_df['weekly_price'])\n",
    "EDLP_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69112cb5-d24f-4c65-b6e4-121b1ff8c737",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample 1 product and feed on two different models\n",
    "# linear regression\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lr = Lasso(alpha=0.05)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_t = lr.predict(X_train)\n",
    "# calculate the RMSE\n",
    "rmse_t = np.sqrt(mean_squared_error(y_train, y_pred_t))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('LR Train RMSE:{0:.3f}'.format(rmse_t))\n",
    "print('LR Test RMSE:{0:.3f}'.format(rmse))\n",
    "# catboost regression\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lr = Lasso(alpha=0.05)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_t = lr.predict(X_train)\n",
    "# calculate the RMSE\n",
    "rmse_t = np.sqrt(mean_squared_error(y_train, y_pred_t))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Tree Train RMSE:{0:.3f}'.format(rmse_t))\n",
    "print('Tree Test RMSE:{0:.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e0a2fd-7777-4509-b4f6-948f98fabfc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "EDLP - pit/qit (consider competitors)\n",
    "\n",
    "Optimize price for each product and predict demand in each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeeeee1a-68f0-4676-914a-f11b2a1da9f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def optimization(sample, j):\n",
    "    sample['price_opt'] = j \n",
    "    sample['P_opt'] = sample['price_opt']/sample['competitor_weekly_price']\n",
    "    # standardized features considering their different magnitude\n",
    "    sample['Seasonality'] = (sample['category_revenue'] - sample['category_revenue'].mean())/sample['category_revenue'].std()\n",
    "    sample = sample.drop(sample[sample['Target'] == np.inf].index)\n",
    "    X = sample[['P','sub','comp','Seasonality']]\n",
    "    y = sample[['Target']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf = cbt.CatBoostRegressor(iterations=30,depth=5, learning_rate=0.2,loss_function=\"RMSE\",verbose=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_clf = clf.predict(X_test)\n",
    "    y_pred_t_clf = clf.predict(X_train)\n",
    "    RMSE_TRAIN_clf = math.sqrt(mean_squared_error(y_train, y_pred_t_clf))\n",
    "    RMSE_TRAIN_clf = float('{0:.2f}'.format(RMSE_TRAIN))\n",
    "    RMSE_TEST_clf = math.sqrt(mean_squared_error(y_test, y_pred_clf))\n",
    "    RMSE_TEST_clf = float('{0:.2f}'.format(RMSE_TEST))\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    y_pred_t_lr = lr.predict(X_train)\n",
    "    RMSE_TRAIN_lr = math.sqrt(mean_squared_error(y_train, y_pred_t_lr))\n",
    "    RMSE_TRAIN_lr = float('{0:.2f}'.format(RMSE_TRAIN))\n",
    "    RMSE_TEST_lr = math.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "    RMSE_TEST_lr = float('{0:.2f}'.format(RMSE_TEST))\n",
    "   # print('CAT Train RMSE:', RMSE_TRAIN_clf,'CAT Test RMSE:', RMSE_TEST_clf,'\\nLR Train RMSE:', RMSE_TRAIN_lr,'LR Test RMSE:', RMSE_TEST_lr,)\n",
    "    X_opt = sample[['P_opt','sub','comp','Seasonality']]\n",
    "    X_opt = X_opt.rename(columns = {'P_opt':'P'})\n",
    "    # choose model with lower rmse\n",
    "    if RMSE_TEST_lr<RMSE_TEST_clf:\n",
    "        y_opt = lr.predict(X_opt)\n",
    "    else:\n",
    "        y_opt = clf.predict(X_opt)\n",
    "    sample['D_opt'] = sample.max_demand/(1+np.exp(-y_opt.flatten()))\n",
    "    Re_EDLP.append((sample['D_opt']*sample['price_opt']).sum())\n",
    "    return sample, Re_EDLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2623b7ed-7d94-4574-9e5d-110a16e712b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EDLP_opt = pd.DataFrame(columns = ['prod_id','original_revenue','optimized_revenue','optimized_price','price_change','revenue_change'])\n",
    "for i in EDLP_df.prod_id.unique():\n",
    "    Re_EDLP =  []\n",
    "    i =  EDLP_df.prod_id.unique()[1]\n",
    "    sample = EDLP_df[EDLP_df.prod_id==i]\n",
    "    sample['Target'] = np.log(EDLP_df.weekly_demand/(EDLP_df.max_demand-EDLP_df.weekly_demand))\n",
    "    # consider competitors\n",
    "    sample['P'] = EDLP_df.weekly_price/EDLP_df.competitor_weekly_price\n",
    "    # every day low price: iterate between lowest price and highest price of the product, make elasticity as close to -1 as possible and find the price that optimized the revenue of stores; aggregate the optimized revenue across all the stores of each product\n",
    "    lower = round(sample.weekly_price.min(),2)-1\n",
    "    upper = round(sample.weekly_price.max(),2)+1\n",
    "    p_range =  np.arange(lower, upper, 0.1)\n",
    "    for j in p_range:\n",
    "        sample, Re_EDLP = optimization(sample,j)\n",
    "    orrev = float('{0:.2f}'.format((sample['weekly_demand']*sample['weekly_price']).sum()))\n",
    "    oprev = float('{0:.2f}'.format((np.max(Re_EDLP))))\n",
    "    rbest = p_range[np.array(Re_EDLP).argmax()]\n",
    "    up = '{0:.2f}%'.format((oprev-orrev)/orrev*100)\n",
    "    pp = '{0:.2f}%'.format((rbest- sample.weekly_price.mean())/sample.weekly_price.mean()*100)\n",
    "    it_row = [i,orrev, oprev,rbest,pp,up]\n",
    "    it_row = pd.DataFrame([it_row], columns = ['prod_id','original_revenue','optimized_revenue','optimized_price','price_change','revenue_change'])\n",
    "    EDLP_opt = pd.concat([EDLP_opt,it_row])\n",
    "EDLP_opt['prod_id'] = EDLP_opt['prod_id'].astype('long')\n",
    "EDLP_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "401e8f03-019e-42b2-a86d-51b20ce7f333",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# show the demand-price\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(p_range, Re_EDLP, '-o',markevery=[np.argmax(Re_EDLP)])\n",
    "plt.title('product %i price - demand'%i)\n",
    "plt.xlabel('EDLP')\n",
    "plt.ylabel('Revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87681fcb-98c8-4df6-9ad1-7a08c131f6b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# orgnize the demand/revenue change of each store\n",
    "trxn_EDLP = trxn.select('prod_id','store_id','sales_amt','sales_qty').join(spark.createDataFrame(EDLP_opt).drop('price_change','original_revenue','revenue_change'), on='prod_id')\n",
    "prod_store_revenue = trxn_EDLP.groupBy(['prod_id','store_id']).agg(sum('sales_amt').alias('revenue'),sum('sales_qty').alias('demand'))\n",
    "prod_revenue = trxn_EDLP.groupBy(['prod_id']).agg(sum('sales_amt').alias('total_revenue'))\n",
    "prod_store_revenue = prod_store_revenue.join(prod_revenue, on='prod_id')\n",
    "prod_store_revenue = prod_store_revenue.withColumn('store_ratio',round(prod_store_revenue.revenue/prod_store_revenue.total_revenue,2))\n",
    "trxn_EDLP = trxn_EDLP.join(prod_store_revenue, on=['prod_id','store_id'])\n",
    "trxn_EDLP = trxn_EDLP.withColumn('Opt_Price',round(trxn_EDLP.optimized_price,2))\n",
    "trxn_EDLP = trxn_EDLP.withColumn('revenue',round(trxn_EDLP.revenue,2))\n",
    "trxn_EDLP = trxn_EDLP.withColumn('total_revenue',round(trxn_EDLP.total_revenue,2))\n",
    "trxn_EDLP = trxn_EDLP.withColumn('Opt_Revenue',round(trxn_EDLP.optimized_revenue*trxn_EDLP.store_ratio,2))\n",
    "trxn_EDLP = trxn_EDLP.withColumn('Revenue_Change',format_string(\"%.2f%%\", (trxn_EDLP.Opt_Revenue-trxn_EDLP.revenue)/trxn_EDLP.revenue*100))\n",
    "trxn_EDLP = trxn_EDLP.withColumn('Opt_demand',trxn_EDLP.Opt_Revenue/trxn_EDLP.Opt_Price)\n",
    "trxn_EDLP = trxn_EDLP.withColumn('Opt_demand',trxn_EDLP.Opt_demand.cast(\"integer\"))\n",
    "trxn_EDLP = trxn_EDLP.withColumn('Demand_Change',format_string(\"%.2f%%\", (trxn_EDLP.Opt_demand-trxn_EDLP.demand)/trxn_EDLP.demand*100))\n",
    "trxn_EDLP.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c378156-d140-4dfe-b6d7-be55679200c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hi-Lo - pit \n",
    "\n",
    "Optimize price for each product and predict demand in each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca47a91-c197-4eb4-9ffd-9cba7693b8f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill in missing value \n",
    "HiLo_df['sub'] = HiLo_df['sub'].fillna(HiLo_df['sub'].mean())\n",
    "HiLo_df['competitor_weekly_price'] = HiLo_df['competitor_weekly_price'].fillna(HiLo_df['weekly_price']) \n",
    "HiLo_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75edb66f-e516-4e88-a584-a7be6f675487",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hi-Lo similar as EDLP\n",
    "def optimization_hilo(report, sample, j):\n",
    "    sample['price_opt'] = j*sample['competitor_weekly_price'].mean()\n",
    "    # standardization\n",
    "    sample['Seasonality'] = (sample['category_revenue'] - sample['category_revenue'].mean())/sample['category_revenue'].std()\n",
    "    sample['P_opt'] = sample['price_opt']/sample['competitor_weekly_price']\n",
    "    sample = sample.drop(sample[sample['Target'] == np.inf].index)\n",
    "    X = sample[['P','sub','comp','Seasonality']]\n",
    "    y = sample[['Target']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf = cbt.CatBoostRegressor(iterations=30,depth=5, learning_rate=0.2,loss_function=\"RMSE\",verbose=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_clf = clf.predict(X_test)\n",
    "    y_pred_t_clf = clf.predict(X_train)\n",
    "    RMSE_TRAIN_clf = math.sqrt(mean_squared_error(y_train, y_pred_t_clf))\n",
    "    RMSE_TRAIN_clf = float('{0:.2f}'.format(RMSE_TRAIN))\n",
    "    RMSE_TEST_clf = math.sqrt(mean_squared_error(y_test, y_pred_clf))\n",
    "    RMSE_TEST_clf = float('{0:.2f}'.format(RMSE_TEST))\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    y_pred_t_lr = lr.predict(X_train)\n",
    "    RMSE_TRAIN_lr = math.sqrt(mean_squared_error(y_train, y_pred_t_lr))\n",
    "    RMSE_TRAIN_lr = float('{0:.2f}'.format(RMSE_TRAIN))\n",
    "    RMSE_TEST_lr = math.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "    RMSE_TEST_lr = float('{0:.2f}'.format(RMSE_TEST))\n",
    "    X_opt = sample[['P_opt','sub','comp','Seasonality']]\n",
    "    X_opt = X_opt.rename(columns = {'P_opt':'P'})\n",
    "        # choose model with lower rmse\n",
    "    if RMSE_TEST_lr<RMSE_TEST_clf:\n",
    "        y_opt = lr.predict(X_opt)\n",
    "    else:\n",
    "        y_opt = clf.predict(X_opt)\n",
    "    print(lr.coef_)\n",
    "    sample['D_opt'] = sample.max_demand/(1+np.exp(-y_opt.flatten()))\n",
    "    # after predicting demand using price, fit the features and target to constant-elasticity model for the currenct elasticity. keep elasticity>-1\n",
    "    result = smf.ols('np.log(D_opt) ~ np.log(price_opt)+np.log(sub)+np.log(comp)+np.log(Seasonality)', data=sample).fit()\n",
    "    slope = float('{0:.3f}'.format(result.params[1]))\n",
    "    Re = float('{0:.2f}'.format((sample['D_opt']*sample['price_opt']).sum()))\n",
    "    price =  float('{0:.2f}'.format(sample['price_opt'].unique()[0]))\n",
    "    ogp = sample.weekly_price.mean()\n",
    "    new_row = pd.DataFrame(np.array([[Re,slope,price,ogp]]), columns=['Revenue','elasticity','price','og_price'])\n",
    "    report = pd.concat([report,new_row])\n",
    "    return sample, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "780799eb-c52c-440c-94de-ce2cecf22f17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Increase price, make elasticity as close to -1 as possible; aggregate the optimized revenue across all the stores of each product\n",
    "HiLo_opt = pd.DataFrame(columns = ['prod_id','original_revenue','optimized_revenue','optimized_price','price_change','revenue_change'])\n",
    "for i in HiLo_df.prod_id.unique():\n",
    "    sample = HiLo_df[HiLo_df.prod_id==i]\n",
    "    sample['Target'] = np.log(HiLo_df.weekly_demand/(HiLo_df.max_demand-HiLo_df.weekly_demand))\n",
    "    sample['P'] = HiLo_df.weekly_price/HiLo_df.competitor_weekly_price\n",
    "    # adjust price\n",
    "    p_range =  np.arange(1.01, 1.5, 0.01)\n",
    "    for j in p_range:\n",
    "        sample, report = optimization_hilo(report,sample,j)\n",
    "    orrev = float('{0:.2f}'.format((sample['weekly_demand']*sample['weekly_price']).sum()))\n",
    "    Rmax = report[report.elasticity>-1].Revenue.max()\n",
    "    oprev = float('{0:.2f}'.format(Rmax))\n",
    "    rbest = report[report.Revenue==Rmax].price[0]\n",
    "    pp = '{0:.2f}%'.format(((rbest-report.og_price.unique()[0])/report.og_price.unique()[0])*100)\n",
    "    up = '{0:.2f}%'.format((oprev-orrev)/orrev*100)\n",
    "    it_row = np.array([[i,orrev, oprev,rbest, pp, up]])\n",
    "    it_row = pd.DataFrame(it_row, columns = ['prod_id','original_revenue','optimized_revenue','optimized_price','price_change','revenue_change'])\n",
    "HiLo_opt = pd.concat([HiLo_opt,it_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe10694a-7946-450f-b627-dac030667286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# orgnize the demand/revenue change of each store\n",
    "trxn_HiLo = trxn.select('prod_id','store_id','sales_amt','sales_qty').join(spark.createDataFrame(HiLo_opt).drop('price_change','original_revenue','revenue_change'), on='prod_id')\n",
    "prod_store_revenue_HiLo = trxn_HiLo.groupBy(['prod_id','store_id']).agg(sum('sales_amt').alias('revenue'),sum('sales_qty').alias('demand'))\n",
    "prod_revenue_HiLo = trxn_HiLo.groupBy(['prod_id']).agg(sum('sales_amt').alias('total_revenue'))\n",
    "prod_store_revenue_HiLo = prod_store_revenue_HiLo.join(prod_revenue_HiLo, on='prod_id')\n",
    "prod_store_revenue_HiLo = prod_store_revenue_HiLo.withColumn('store_ratio',round(prod_store_revenue_HiLo.revenue/prod_store_revenue_HiLo.total_revenue,2))\n",
    "trxn_HiLo = trxn_HiLo.join(prod_store_revenue_HiLo, on=['prod_id','store_id'])\n",
    "trxn_HiLo = trxn_HiLo.withColumn('Opt_price',round(trxn_HiLo.optimized_price,2))\n",
    "trxn_HiLo = trxn_HiLo.withColumn('revenue',round(trxn_HiLo.revenue,2))\n",
    "trxn_HiLo = trxn_HiLo.withColumn('total_revenue',round(trxn_HiLo.total_revenue,2))\n",
    "trxn_HiLo = trxn_HiLo.withColumn('Opt_Revenue',round(trxn_HiLo.optimized_revenue*trxn_HiLo.store_ratio,2))\n",
    "trxn_HiLo = trxn_HiLo.withColumn('Revenue_Change',format_string(\"%.2f%%\", (trxn_HiLo.Opt_Revenue-trxn_HiLo.revenue)/trxn_HiLo.revenue*100))\n",
    "trxn_HiLo = trxn_HiLo.withColumn('Opt_Demand',trxn_HiLo.Opt_Revenue/trxn_HiLo.Opt_price)\n",
    "trxn_HiLo = trxn_HiLo.withColumn('Opt_Demand',trxn_HiLo.Opt_Demand.cast(\"integer\"))\n",
    "trxn_HiLo = trxn_HiLo.withColumn('Demand_Change',format_string(\"%.2f%%\", (trxn_HiLo.Opt_Demand-trxn_HiLo.demand)/trxn_HiLo.demand*100))\n",
    "trxn_HiLo = trxn_HiLo.select(['prod_id','store_id','Opt_price','demand','revenue','Opt_Demand','Opt_Revenue','Demand_Change','Revenue_Change']).distinct()\n",
    "trxn_HiLo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ecacc79-00fc-4bdd-9b5c-c530914139b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Calculate the total revenue increasement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dae4256-f7fc-4047-8659-324e46bf81c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_revenue_change = (EDLP_opt.optimized_revenue.astype('float')-EDLP_opt.original_revenue.astype('float')).sum()+(HiLo_opt.optimized_revenue.astype('float')-HiLo_opt.original_revenue.astype('float')).sum()\n",
    "print('We help ASCE increase revenue by',total_revenue_change,'.')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pricing Project",
   "notebookOrigID": 4130656934301083,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
